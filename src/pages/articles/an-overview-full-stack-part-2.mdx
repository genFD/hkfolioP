import { ArticleLayout } from '@/components/ArticleLayout'
import Image from 'next/image'
import requestserver from './assets/requestserver.png'
import webserver from './assets/webserver.png'
import nginx from './assets/nginx.png'
import nginxconfig from './assets/nginx-config.png'
import codetools from './assets/codetools.webp'
import basedirreq from './assets/base-dir-req.png'
import createarecord from './assets/createnewarecord.png'
import locationblock from './assets/location-block.png'
import hellownginorld from './assets/hellownginorld.png'
import monvsmicro from './assets/monolith-vs-microservices.png'
import ngninxrequestnode from './assets/request-ngnix-node.png'
import nginxt from './assets/ngninx-t.png'
import badgateway from './assets/badgateway.png'
import noderunning from './assets/node-running.png'
import virtvscont from './assets/virt-vs-contain.png'
import pm2 from './assets/pm2-app.png'
import pm2su from './assets/pm2-startup.png'
import pm2status from './assets/pm2status.png'
import openports from './assets/openports.png'
import open from './assets/open.png'
import permission from './assets/permission.png'
import chmod from './assets/examplechmod.png'
import cron from './assets/cron.png'
import cronguru from './assets/crontabguru.png'
import cronguru2 from './assets/crontabguru2.png'
import syslog from './assets/syslog.png'
import redirectnginx from './assets/redirect.png'
import createrserv from './assets/server.conf.png'
import editconf from './assets/editconf.png'
import subdomain from './assets/subdomain.png'
import websockconf from './assets/websockconf.png'
import vscodestartserver from './assets/vscodestartserver.png'
import websocketbrowser from './assets/websocketbrowser.png'
import wbsockconnect from './assets/websocketsbrowserconnect.png'
import wbsockconnect2 from './assets/wbsocketconnect2.png'
import newfileserver from './assets/newfileserver.png'
import reldb1 from './assets/reladb1.png'
import reldb2 from './assets/reldb2.png'
import reldb3 from './assets/reldb3.png'
import visitors from './assets/websocketsbrwoser.png'
import logclients from './assets/logclients.png'
import successcertbot from './assets/successcertbot.png'
import certbotnginx from './assets/certbotnginx.png'
import https from './assets/https.png'
import protocolhttp1 from './assets/protocolhttp1.png'
import http2certbot from './assets/http2certbot.png'
import http2protocol2 from './assets/http2protocol2.png'
import htmlcountries from './assets/htmlgetcountries.png'
import countries from './assets/countries.png'
import dockerrun from './assets/dockerrun.png'
import secdinstance from './assets/secdinstance.png'

import loadbalancer from './assets/load-balancer.png'

import Illustration from './assets/illustration.png'

export const meta = {
  author: 'Hermann Kanga',
  date: '2023-09-05',
  title: 'An overview of Full stack developement - Part II',
  description:
    'In this second part, we will get hands-on with building and deploying a web application from scratch.',
}

export default (props) => <ArticleLayout meta={meta} {...props} />

<Image src={codetools} alt="" />

## 📖 Table of contents

Part 2:

- Application setup
- Security
- Continous Integration and Delivery
- Loggins and redirection
- Subdomains
- Databases
- Containers

## Application setup

If we navigate to our server's domain in the browser right now, this is what we see :

<Image src={requestserver} alt="server in" />

This is because the server doesn't know how to respond to a request. Let's make our server respond to HTPP requests.

To do this we need a web server. There are many options out there (check out this <a target="_blank" href="http://www.digitalocean.com">link</a> for more information) but we will choose <a target="_blank" href="https://nginx.org/en/">NGINX</a>.

### Why do we need a web server?

When an HTTP request comes in, we need a web server to route that request to the right place (database, application or another server)

<Image src={webserver} alt="server in" />

We technically don't need a web server to route our requests, but it's not a good practice.

Let's install and set up NGINX:

1. SSH into your server and run this command to install NGINX :

```bash
sudo apt install nginx
```

2. Start Nginx

```bash
sudo service nginx start
```

3. Navigate to your server in the browser

After running these commands, if you try to navigate to your server's domain in the browser again you will see this page :

<Image src={nginx} alt="server in" />

We've now officially hooked up the domain to the server. This is a good starting place.

Let's check the NGINX default server configuration by running the following command :

```bash
less /etc/sites-available/default
```

You should see :

<Image src={nginxconfig} alt="server in" />

- root /var/www/html : This is the base directory for requests, the welcome page above is located in this directory.

<Image src={basedirreq} alt="server in" />

- location : Location blocks are used to decide how to process the request URI (the part of the request that comes after the domain name or IP address/port)

<Image src={locationblock} alt="server in" />

- directives : In this example, the directive is to try to serve an HTML page and if not there serve the 404 page. We will use another directive called `proxy_pass` to route our request into node from NGINX.

Let's edit the default homepage :

```bash
cd /var/www/html
vi index.html
# add hello world and save and exit
```

if you go back to the browser and refresh the page you should see :

<Image src={hellownginorld} alt="server in" />

{/* NGINX is great, but let's move into a domain, we are much more familiar with. We are going to use nodejs to host our application. The request will come to the server, Nginx will proxy it to the nodejs. */}

### Setup NGINX as a reverse proxy

A reverse proxy is the recommended method to expose an application server to the internet.
First, let's install Node.js on our server to create a basic Node.js application :

<Image src={ngninxrequestnode} alt="server in" />

These are the steps to follow :

Here is the link to <a href="https://github.com/nodesource/distributions/blob/master/README.md#debinstall">
install node </a> on linux.

1. Download and import the Nodesource GPG key :

```bash
sudo apt-get update
sudo apt-get install -y ca-certificates curl gnupg
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg
```

2. Create deb repository :

```bash
NODE_MAJOR=20
echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main" | sudo tee /etc/apt/sources.list.d/nodesource.list
```

3. Install node :

```bash
sudo apt-get update
sudo apt-get install nodejs -y
```

4. Install git (If git not already installed) :

```bash
sudo apt install git
```

Next, let's create our application :

1. Change ownership of /www

```bash
sudo chown -R $USER:$USER /var/www
```

2. Make an application directory

```bash
mkdir /var/www/app
```

3. Initialize an empty git repository

```bash
cd app
git init
```

4. Create application files

```bash
npm init -y
touch app.js
```

5. Create a basic nodejs server in app.js

```bash
vi app.js
```

Copy-paste this :

```vim
const http = require('http')
const PORT = 8000

const server = http.createServer((req, res) =>
res.write("Hello from basic nodejs server")
res.end()
})

server.listen(PORT)
console.log(`server is listening to port ${PORT}))
```

{/* check for example our to write article explaining instructions : */}

{/* https://www.digitalocean.com/community/tutorials/how-to-configure-nginx-as-a-reverse-proxy-on-ubuntu-22-04 */}

6. Setup proxy pass

Proxy pass is an NGINX directive that allows us to route any requests that comes to our server to our node.js server.

```bash
sudo vi /etc/nginx/sites-enabled/hk
```

Copy-paste the following configuration to create a virtual server block :

```vim
server {
    listen  80;
    listen  [::] : 80 default_server
    root /var/www/html
    index index.html
    server_name <your_domain>
    location / {
              proxy_pass http://127.0.0.1:8000
    }
}
```

- We listening on port 80
- Serving an index.html in `/var/www/html` directory
- Renamed our server
- Told nginx to reroute the request to localhost:port8000 where our node js server is running

Now we're going to point NGINX to our new server:

```bash
sudo vi /etc/nginx/ngnix.conf
```

Modify this line `/etc/nginx/sites-enabled/ ` to include :

```vim
/etc/nginx/sites-enabled/<server_name>
```

To validate your configuration file you can run the following command :

```bash
nginx -t
```

if everything is correct you should get the following output:

<Image src={nginxt} alt="server in" />

let's check nginx status and restart

```bash
sudo service nginx status
sudo service nginx restart
```

In the terminal run the following command:

```bash
cd /var/www/app
node app.js
```

Refresh the page in the browser you should see the following:

<Image src={noderunning} alt="server in" />

Congratulations, we have :

- bought a server on digital ocean
- bought a domain
- connected the domain to our digital ocean server
- created a node.js app
- setup NGINX to reroute requests coming to the server to your node.js app

With this in place, we can build a full web page, accessible via our domain (mine is **hkgetsit.tech**)

To make sure our webpage stays up and running even when we close the shell or exit the terminal, we're going to install a program called **PM2**.
PM2 is a powerful process manager for Node.js.
The start script sets up PM2 as a service under the [init](https://linuxtldr.com/init-linux/) system. When the server restarts, it will automatically restart PM2, which will then restart all the Node.js applications/processes it is managing.

1. Install pm2

```bash
sudo npm i -g pm2
```

2. start pm2

```bash
pm2 start app.js
```

you should get this output:

<Image src={pm2} alt="server in" />

3. setup autostart

```bash
pm2 startup
pm2 save
```

copy paste the output command and execute it

you should see the following output:

<Image src={pm2su} alt="server in" />

To check if all your Node.js processes are running under PM2, we can run the following command:

```bash
pm2 ls
#or
pm2 status
```

<Image src={pm2status} alt="server in" />

Now our if server reboot, **pm2** is going to bring up our node app automatically.

### Creating a git repo

Right now, our node app is only accesible on the server, let's create a git repository to be able to access our node js app on our local machine.

1. On <a href="https://github.com/">github</a> create a git repository

2. Log in to the server and generate an SSH key called **gh_key**

```bash
cd ~/.ssh
ssh-keygen
```

3. Add the ssh key to github following <a href="https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account">
   these instructions:</a>

4. Make sure you git uses your new SSH key

```bash
vi ~/.ssh/config
#add this to your configuration

Host github.com
Hostname github.com
IdentityFile ~/.ssh/gh_key
```

5. Push the repository to github.

```bash
git push -u origin main
```

We've pushed our code to github, if everything is working correctly, you should be able to pull the code from the repo to your local machine.

## Security

**What could someone do if they gain access to your server?** :

- Use your server as a bot to attack other users infrastructures.
- People can connect to your Github via your server.
- People can connect to your server and delete log files making it impossible to know that your machine was compromised.

**How can we secure our application?** :

Security is paramount when it comes to servers. These are some of the measures that we can take to protect your server. For example :

- Using SSH : we don't use password
- Setup firewalls : a firewall restrict access to activities
- Updates : Keep the software up to date because if there's a hack or a breach you want to ensure your software is always running the latest version
- Two factor authentication
- VPN

In the next section, we will focus on setting up firewalls. But Before getting into firewalls, we need to understand what a port is. By defintion, a port is a communication endpoint that maps to specific process or network service. Ports are amazing because they allow us to run multiple services on one IP address.

To take a look at the well known ports, run this command :

```bash
less /etc/services
```

Ports are an opening in your computer and because of that, they represent a entry point for hackers.

Let's run these commands to take a look at our open ports on our machine. We are going to use a program called `nmap`. :

- Install nmap

```bash
sudo apt install nmap
```

- Run nmap

```bash
nmap  <your_server_ip>
```

- Extra service/version information

```bash
nmap -sV <your_server_ip>
```

<Image src={openports} alt="server in" />

We can see that Port 22 (ssh), Port 80(http) and Port 8000(http-alt) are open.

### Firewall

> A firewall is a network security device that monitors incoming and outgoing network traffic and decides whether to allow or block specific traffic based on a defined set of security rules. [Source](https://www.cisco.com/c/en/us/products/security/firewalls/what-is-a-firewall.html)

<a
  target="_blank"
  href="https://www.cisco.com/c/en/us/products/security/firewalls/what-is-a-firewall.html"
>
  Check out cisco docs on firewalls
</a>

For our example, we will use a program called <a href="https://help.ubuntu.com/community/UFW">
UFW </a> or uncomplicated firewall.

The commands are really simple :

- ufw allow
- ufw deny
- ufw reject

Let's go ahead and use **UFW**. The first thing to do is to allow **UFW** to take over the management of our firewall. We will then allow ssh, http, and finally enable the firewall :

- Check firewall status

```bash
sudo ufw status
```

The status will probably be inactive which is fine.

- Allow ssh

```bash
sudo ufw allow ssh
```

- Allow http

```bash
sudo ufw allow http
```

- Enable firewall

```bash
sudo ufw enable
```

If we run `sudo ufw status` again we should see the open ports :

<Image src={open} alt="server in" />

Our firewall is set up, what else can we do?

If we ever needed to block http connection (for example to only allow https connections), we could run this command :

- Block http

```bash
sudo ufw reject http
```

### Keeping our application up to date

Another solution to keep our application secure is keeping our software and packages up to date. We will use another package called `unattended-upgrades` to allow upgrade to run on the system at a periodic function.

1. Install unattended-upgrades

```bash
sudo apt install unattended-upgrades
```

2. Enable upgrades

```bash
# enable us to run it in the background
sudo dpkg-reconfigure --priority=low unattended-upgrades
```

## Continous integration and delivery (CI/CD)

Continuous integration is an important DevOps concept. The idea is that developers should frequently merge code changes into a central repository where automated tests and builds are run. Continuous integration puts a great emphasis on automated tests to ensure that the application is not broken whenever new commits are integrated into the main branch.

Continuous delivery is an extension of continuous integration since it automatically deploys all code changes to a testing and/or production environment after the build stage. This means that on top of automated testing, you have an automated release process.

For our application, we are going to automate a very simple task :

- Pulling the latest commit from our repository at certain intervals.

In order to do this, we will create a [cron job](https://www.hivelocity.net/kb/what-is-cron-job/) to execute a shell script. This is what a shell script looks like :

```vim
#! /usr/bin/bash

read -p "what is your name" name
echo "have a great day, $name"
```

We can create a file called github.sh to write our script :

```bash
vi github.sh
```

The command to pull from github is `git pull origin <branch_name>`. So, our script would look like this :

```bash
#! /usr/bin/bash
cd /var/www/app
git pull origin main --ff-only
```

This tells git to apply the remote changes only if they can be [fast-forwarded](https://stackoverflow.com/questions/29673869/what-is-git-fast-forwarding).

The first thing we need to do is to change the [permission](https://chmodcommand.com/chmod-700/) so that we can execute the file **github.sh** :

In the terminal run the command :

```bash
chmod 700 github.sh
```

To execute the file, we run the following command:

```bash
cd /var/www/app
./github.sh
# the result should look like this:
#* branch           main       -> FETCH_HEAD
#Already up to date.
```

We wrote a two-line shell script to pull from github, but we want to execute it at certain interval, that's where cron job comes into play.

For our cron job, we need :

1. a timer
2. a path to the script we want to run.

<Image src={cron} alt="cron" />

<a href="https://crontab.guru/">crontab.guru</a> is a website that allows us to generate
the timer for our cronjob. For example if we want to run a cronjob at minute ten
on tuesday we can write the following:

<Image src={cronguru} alt="cron" />

for our app, we want our cronjob to run every two minute (you can change it later if you want), so we can write the following:

<Image src={cronguru2} alt="cron" />

To edit our cron job, we can run this command:

```bash
crontab -e
# you should see the following
Select an editor.  To change later, run 'select-editor'.
  1. /bin/nano        <---- easiest
  2. /usr/bin/vim.basic
  3. /usr/bin/vim.tiny
  4. /bin/ed


# select vim by choosing option 2

Choose 1-4 [1]: 2
```

In the editor, you can write the following:

```vim
*/2 * * * * sh /var/www/app/github.sh 2>&1 | logger -t github.sh
0 22 * * 1-5 sh /var/www/app/github.sh 2>&1 | logger -t github.sh

```

Save it!

We piped our output into syslog, so to check if it's running we can execute this command :

```bash
sudo tail -f /var/log/syslog
```

we should see something like:

<Image src={syslog} alt="cron" />

We can see that the script was executed.

With that in place, everytime we make a change it will be pull down automatically to our server.

## Loggins and Redirection

When it comes to log files, there's a few different ways of reading it :

- `tail` : output the last part of the file
- `head` : output the first part of the file
- `less` : output one page at a time
- `cat` : output entire file

The most common logs are :

- syslog
- auth.log
- nginx/access.log

Logs are always located in **/var/log**

In unix every command has :

- an input called **standard in**
- an output called **standard out**

(More on that [here](https://unix.stackexchange.com/questions/31334/what-is-meant-by-connecting-stdout-and-stdin))

And every function take the same arguments which allows to chain commands.

The most common commands for redirection are:

- `|` : read from standard output
- `>` : write standard output to a file
- `>>` : append standard output to a file
- `<` : read from standard input
- `2>&1` : redirect both standard error and standard output

To find things we can use the following :

- `find` : search file names
- `grep` : search file content

For example :

```bash
find /bar -name foot.txt
```

- find : command
- `/bar` : the directory
- `-name` : option
- `foo.txt` : file

or :

```bash
grep -i 'hello' /var/www
```

- grep : command
- `-i` : option
- `-hello` : search term
- `/var/www` : directory

for example, this command is useful to find the running processes

```bash
ps aux | grep node
```

## Creating a subdomain

Subdomains are useful for developement purposes. For example, a development version of a website can be located at **dev.website.com** while the production version continues to run at **website.com** giving developers the flexibility to make modifications as they wish without breaking the production version.

We're going to create a subdomain called **blog.hkgetsit.tech** :

1. The first thing to do is to create a new A record on <a target="_blank" href="http://www.digitalocean.com">digitalocean.com</a> :

<Image src={createarecord} alt="cron" />

{/* 1. Create a new subdomain called `blog` */}

{/* - create an A record */}
{/* - create a server */}

{/* 2. Update nginx.conf */}
{/* 3. Restart nginx */}

2. Create a new nginx configuration file for our subdomain. Navigate to `/etc/nginx/sites-enabled` and execute the following command :

```bash
sudo vi blog._your_domain_.
```

and edit your file to look like this :

<Image src={createrserv} alt="cron" />

3. Update your main nginx.conf file :

```bash
cd /etc/nginx/
sudo vi nginx.conf
```

edit your file by adding a new "include" line for "blog.your_domain"

<Image src={editconf} alt="cron" />

and finally restart our NGINX server :

```bash
sudo service nginx restart
```

Check out this <a target="_blank" href="http://stackoverflow.com/questions/35868976/ddg#51684856">link</a> if you run into an error.

As always run `nginx -t` to validate your configuration. If you open the browser and navigate to your subdomain, you should see:

<Image src={subdomain} alt="cron" />

## Databases

> A database is an organized collection of structured information, or data, typically stored electronically in a computer system. A database is usually controlled by a database management system (DBMS). Together, the data and the DBMS, along with the applications that are associated with them, are referred to as a database system, often shortened to just database. [Source](https://www.oracle.com/database/what-is-database/)

Another way to think about this is that it is a place to save your application's data (state) so that it can be retrieved later. For example, let's say you are building a website where users can see or download cocktail recipes, you would certainly need a database to store and **query** the list of recipes. A query is a command you send to a database to get it to do something, whether that's to get information out of the database or to add/update/delete something in the database.

There are two main types of databases :

- **Relational databases (often called RDBMS or SQL databases)** : organizes data into rows and columns, which collectively form a table. Data is structured across multiple tables, which can be joined together via a primary key or a foreign key (establishing relationships between tables) . Examples of relational databases are : SQL, mySQL, SQLITE etc..

- **Non-relational databases (often called NoSQL)** : aim to solve for the flexibility and scalability issues inherent in relational models which are not ideal for unstructured data formats, like text, video, and images. These types of databases include: Key-value store, Document store etc.. Examples of non-relational databases are : MongoDB, Cassandra, Couchbase, ReThinkDB etc..

For our application, we're going to create a database using [Sqlite](https://www.sqlite.org/index.html), a lightweight SQL database. The goal is to store a list of countries that we are going to be able to retrieve and display. Check out the <a href="https://github.com/mapbox/node-sqlite3/wiki/API">sqlite3 library API docs</a>

Relational databases are made up of three different components:

- Table : in our example, our table will be `COUNTRIES`
- Fields or columns : we will have two columns `name` and `time`
- Rows or records: we will have three countries, so this table will have three records

Here are the steps to follow :

1. Open up vs code (or the editor of your choice), create a directory and clone the code from your git repository :

```bash
git clone <your_repo_link>
```

2. Run the following command to installl **sqlite3** and **express**

```bash
npm i sqlite3
npm i express
```

- In `app.js`, import sqlite and express :

```js
const sqlite = require('sqlite3')
const express = require('express')
```

- Copy the code below, to create a new express server that sends back an html page when users hit our homepage '/' :

```js
app.use(express.json())

app.get('/', function (req, res) {
  res.sendFile('index.html', { root: __dirname })
})

app.listen(PORT, () => {
  console.log(`listening on port ${PORT}`)
})
```

- Still in app.js, create a new database instance and put it in memory :

```js
...
const db = new sqlite.Database(':memory:')
```

- Create a table named `countries`, with two columns named `name` and `time` and insert three values :

```js
...
db.serialize(() => {
  db.run(`CREATE TABLE countries (
       name Text,
       time TEXT
    )
    `)
  db.run(`INSERT INTO countries (name, time)
          VALUES ('Norway', datetime('now')), ('Spain', datetime('now')), ('Germany', datetime('now'))
  `)
})
```

- Create a new route called "/getCountries" to query the database and return all the records :

```js
...
app.get('/getCountries', function (req, res) {
  db.all('SELECT * FROM countries', (err, rows) => {
    res.send(rows)
  })
})

```

- In the "index.html" file, copy the code below :

```html
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>PostgreSQL Sample</title>
    <style>
      .container {
        height: 95vh;
        width: 95vw;
        display: flex;
        align-items: center;
        justify-content: center;
      }

      .button-30 {
        align-items: center;
        appearance: none;
        background-color: #fcfcfd;
        border-radius: 4px;
        border-width: 0;
        box-shadow: rgba(45, 35, 66, 0.4) 0 2px 4px, rgba(45, 35, 66, 0.3) 0 7px
            13px -3px, #d6d6e7 0 -3px 0 inset;
        box-sizing: border-box;
        color: #36395a;
        cursor: pointer;
        display: inline-flex;
        font-family: 'JetBrains Mono', monospace;
        height: 48px;
        justify-content: center;
        line-height: 1;
        list-style: none;
        overflow: hidden;
        padding-left: 16px;
        padding-right: 16px;
        position: relative;
        text-align: left;
        text-decoration: none;
        transition: box-shadow 0.15s, transform 0.15s;
        user-select: none;
        -webkit-user-select: none;
        touch-action: manipulation;
        white-space: nowrap;
        will-change: box-shadow, transform;
        font-size: 18px;
      }

      .button-30:focus {
        box-shadow: #d6d6e7 0 0 0 1.5px inset, rgba(45, 35, 66, 0.4) 0 2px 4px,
          rgba(45, 35, 66, 0.3) 0 7px 13px -3px, #d6d6e7 0 -3px 0 inset;
      }

      .button-30:hover {
        box-shadow: rgba(45, 35, 66, 0.4) 0 4px 8px, rgba(45, 35, 66, 0.3) 0 7px
            13px -3px, #d6d6e7 0 -3px 0 inset;
        transform: translateY(-2px);
      }

      .button-30:active {
        box-shadow: #d6d6e7 0 3px 7px inset;
        transform: translateY(2px);
      }
      .box {
        display: flex;
        flex-direction: column;
        gap: 5px;
      }
      ul {
        list-style-type: none;
      }
      li {
        padding: 5px 10px;
        background-color: #d6d6e7;
        color: black;
        border: 1px solid #dddddd;
      }
    </style>
  </head>
  <body style="background: black">
    <div class="container">
      <div class="box">
        <button class="button-30" role="button" id="btn">get countries</button>

        <ul id="myList"></ul>
      </div>
    </div>

    <script>
      const btn = document.getElementById('btn')
      const list = document.getElementById('myList')
      async function httpGetCountries() {
        try {
          const response = await fetch('http://localhost:8000/getCountries')
          const countries = await response.json()
          console.log(countries)
          for (i = 0; i < countries.length; ++i) {
            let li = document.createElement('li')

            li.innerText = countries[i].name

            list.appendChild(li)
          }
        } catch (error) {
          console.log(error)
          return {
            ok: false,
          }
        }
      }

      btn.addEventListener('click', httpGetCountries)
    </script>
  </body>
</html>
```

- Start up the server and open up the browser at http://localhost:8000/, you should see this page with the button "get countries" :

<Image src={htmlcountries} alt="cron" />

- Click on the button "get countries" to display the list of countries stored in the database :

<Image src={countries} alt="cron" />

Commit your changes to the repository.

## HTTPS

HTTP is made up of two parts :

- the request
- the response

here's how a request looks like :

```bash
GET /http.1
Host: hkgetsit.tech
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/118.0
Accept: */*
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate, br
```

Some common headers are:

- user-agent : the requesting device type
- Accept : what the device type will handle
- Accept : the browser language
- Content-type : The type of media
- Set-cookie : sets stateful information
- X- : typically used for custom headers.

The response looks like this :

```bash
GET /http.1.1 OK
server: nginx/1.1.4 Ubuntu
date: sun nov 18 2020 1:14:13 GMT
Content-Type: application/json
Content-Length: 12
```

Status code indicates the status of an http request. Some common status code are :

- 200 : OK
- 301 : Moved permanently
- 302 : Found (temp redirect)
- 401 : Not authorized
- 500 : Internal server error

status code begining by :

- 1xx : is usually information
- 2xx : success
- 3xx : redirect
- 4xx : client error
- 5xx : server error

All modern browsers will require to use https to secure the request and response traffic.
HTTPS helps us to encrypt the information sent back and forth between the server and the client.

Let's implement https.

To do that we're going to use <a href="https://certbot.eff.org/instructions">certbot</a>.

<Image src={logclients} alt="cron" />

Select the type of nginx and ubuntu 20 and you should see the instructions for setting up nginx :

1. SSH into the server

SSH into the server running your HTTP website as a user with sudo privileges.

2. install snapd

```bash
sudo snap install core
sudo snap refresh core

```

3. Remove certbot-auto and any Certbot OS packages

```bash
sudo apt-get remove certbot
```

4. Install Certbot

```bash
sudo snap install --classic certbot
```

5. Prepare the Certbot command

Execute the following instruction on the command line on the machine to ensure that the certbot command can be run.

```bash
sudo ln -s /snap/bin/certbot /usr/bin/certbot
```

6. Choose how you'd like to run Certbot

Run this command to get a certificate and have Certbot edit your nginx configuration automatically to serve it, turning on HTTPS access in a single step.

```bash
sudo certbot --nginx
```

- Put in your email
- select Yes to read the terms and service
- select yes or no for the newsletter
- leave it blank to have certificate for both the domain and the subdomain

If it's successful you should be able to see this :

<Image src={successcertbot} alt="cron" />

What certbot did is :

- Adjusting our nginx configuration

<Image src={certbotnginx} alt="cron" />

7. Test automatic renewal

The Certbot packages on your system come with a cron job or systemd timer that will renew your certificates automatically before they expire.

```bash
sudo certbot renew --dry-run
```

Finally, we're going to open the port :

```bash
sudo ufw allow https
```

Now if we open up the browser and visit our website, we should see a lock indicating the connection is now secured via https.

<Image src={https} alt="cron" />

And that's it! we've implemented `https`

## HTTP2

HTTP2 does something called multiplexing, which just means we can do multiple things on one connection.

on our website we can see that the protocol used is http1

<Image src={protocolhttp1} alt="cron" />

Let's implement http2

```bash
sudo vi /etc/nginx/sites-enabled/hk
```

add http2 on the listen block like this:

<Image src={http2certbot} alt="cron" />

restart nginx :

```bash
sudo service nginx restart
```

now, let's open up the website again :

<Image src={http2protocol2} alt="cron" />

we can see that the protocol has changed to http2

## Containers

**What are Containers?**

> Containers are packages of software that contain all of the necessary elements to run in any environment. In this way, containers virtualize the operating system and run anywhere, from a private data center to the public cloud or even on a developer’s personal laptop. Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. [Source](https://cloud.google.com/learn/what-are-containers)

Containers exist because they solve an important problem: **how to make sure that software runs correctly when it is moved from one computing environment to another**.

A well-known solution to this problem is **virtualization**. Virtualization allows multiple operating systems to be run completely independently on a single machine. The main benefit of this approach is that the operating system is encapsulated, which means that there are far fewer compatibility problems.

Containerization is an extension of the virtualization approach.
Virtualization uses specialized software called a **Hypervisor** that encapsulates a guest version of the operating system and emulates hardware resources like server CPU, memory, hard disk and other resources so that these resources can be shared across multiple virtual machines. The hypervisor creates and runs multiple instances of an operating system so that multiple operating systems can be run on a single physical machine sharing the hardware resources.
Containerization takes things a little further. It achieves far greater efficiency than virtualization by eliminating the hypervisor and its multiple virtual machines.
Instead of hypervisors, containers are essentially applications, and all their dependencies packaged into virtual containers.
Containers contain not just the application, but everything that it needs to run including runtime, system libraries, etc. Each application shares a single instance of the operating system and runs on the “bare metal” of the server.

<Image src={virtvscont} alt="cron" />

We can see on the diagram above that since the operating system is shared by all containers and because they are much more lightweight than traditional virtual machines, it’s possible to host far more containers on a single host than fully-fledged virtual machines.

**Why do we need containers?**

Besides obvious reasons such as : containers are lightweight, easier for development, easier to manage etc.., the biggest selling point is that containers allow us to decouple the application from the infrastructure. This decoupling allows container-based applications to be deployed easily and consistently, regardless of whether the target environment is a private data center, the public cloud, or even a developer’s personal laptop.

Containers are also one of the key technology that enable us to build **microservices**, which is a software architecture design for building distributed applications. Applications were traditionally built as monolithic pieces of software. Monolithic applications are updated infrequently and changes usually affect the entire application. Adding new features requires reconfiguring and updating the entire stack which can be a costly and cumbersome process delaying the application development.
Microservices architecture was designed to remedy this problem. All services are created individually and deployed separately. This allows for each service to scale or update, without disrupting other services in the application and enables the rapid, frequent and reliable delivery of large, complex applications..

<Image src={monvsmicro} alt="cron" />

**How do we containerize our application?**

[Docker](https://www.docker.com/) is by far the best-known containerization technology.
Docker containers package an application and its dependencies into a single image file, which can be deployed to any platform that supports Docker.
Docker has the advantage of a large user community providing excellent support to users.

These are the steps to follow in order to containerize our application with docker :

- Write a Dockerfile
- Install Docker
- Build a Docker image
- Run a Docker container

1. Writing a Dockerfile :

- Navigate to the app directory:

```bash
cd /var/www/app
```

- Create a dockerfile

```bash
vi dockerfile
```

add the following :

```vim
FROM node:19-alpine3.16
Run mkdir -p /home/node/app/nodes_modules && chown -R node:node /home/node/app/
WORKDIR /home/node/app
COPY --chown=node:node package*.json ./
USER node
Run npm install
COPY --chown=node:node . .
EXPOSE 8000
CMD ["node", "app.js"]

```

- The first line defines what type of OS it's running on : we're using alpine which is lightweight version of linux
- We will create an app directory within the node home directory and changed ownership to the node owner
- We set up a working directory (/home/node/app)
- Next because we're making a docker image from the application we already wrote, we need to copy the package file from our directory to docker container.
- We set the user to NODE
- Install the packages
- We changed the ownership from our current os which is owned by us to node user and copied the rest of our application
- We exposed port 8000 to run our application
- Finally, we run the command : `node app.js` to start the server.

2. Install docker :

```bash
sudo apt install docker.io
```

3. Create docker build :

```bash
docker build -t node-hkgetsit .
```

to see the image we created, we can run the command:

```bash
docker image ls
```

4. Run the docker image in the background :

- first we need to stop the app running on port 8000

```bash
pm2 stop app.js
```

- run the image

```bash
docker run -d -p 8000:8000 node-hkgetsit
```

Now if we visit our website again, we're back to our simple node server :

<Image src={dockerrun} alt="cron" />

We can create another docker instance running on another port :

```bash
docker run -d -p 3001:8000 node-hkgetsit
```

We can see that we have two instances running by running this command

```bash
docker ps
```

You should see something like this:

<Image src={secdinstance} alt="cron" />

and that's the power of docker we can run as many instances as we like on one server.

## Load balancer

{/* <Image src={orchestra} alt="cron" /> */}

{/* In a real world, you can have thousands of containers running. We need something to manage those containers, we call it orchestration. */}
{/* One of the most popular tool, to do it is `Kubernetes`. With kubernetes, we can create hundreds of containers at one time, all running the exaxt instance of our application. */}

Going back to our application, we have two instances running, we can balance between the two of them using a load balancer :

<Image src={loadbalancer} alt="cron" />

if one server is overloaded we would be able to route the request to the one that's runnning lower.
There's different types of algorithm for deciding how to balance between containers :

- Round robin algorithm
- IP hashing
- Random choice
- Least connection
- Least loads

Let's implement a load balancer in nginx. We're going to create an Upstream, to list the servers we want to connect to and then in a server block we'll proxy pass the cluster and nginx will take care of the rest.

1. add server cluster to the nginx configuration

```bash
sudo vi /etc/nginx/nginx.conf
```

and add the following in the http block :

```vim
upstream nodebackend {
  server localhost:8000;
  server localhost:3001;
}
```

2. Proxy pass to the cluster

```bash
sudo vi /etc/nginx/sites-enabled/hk
```

and add the following in the location block :

```vim
location {
  ...
  proxy_pass http://nodebackend
}
```

- restart nginx

```bash
sudo service nginx restart
```

If we did everything correctly, you should be able to visit the website.

We're now have a fully functionning web application
